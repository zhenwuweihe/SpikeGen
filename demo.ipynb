{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "from datasets.nerf_dataset import get_nerf_dataloader\n",
    "from datasets.online_process_dataset import get_online_process_dataloader, SpikeConverter\n",
    "from models.spike_mar import spike_mar_base, spike_mar_large, spike_mar_huge\n",
    "# from models.vae import AutoencoderKL\n",
    "from diffusers import AutoencoderKL\n",
    "from util import misc\n",
    "from util.metrics import compute_img_metric\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.misc import AverageMeter\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "def load_model(args):\n",
    "    \"\"\"Load all model components\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    vae2d = AutoencoderKL.from_pretrained(args.rgb_vae_path).eval()\n",
    "    \n",
    "    model_func = globals()[args.model]\n",
    "    model = model_func(\n",
    "        img_h=args.img_h,\n",
    "        img_w=args.img_w,\n",
    "        vae_stride=args.vae_stride,\n",
    "        patch_size=args.patch_size,\n",
    "        vae_embed_dim=args.rgb_vae_embed_dim,\n",
    "        attn_dropout=args.attn_dropout,\n",
    "        proj_dropout=args.proj_dropout,\n",
    "        buffer_size=args.buffer_size,\n",
    "        diffloss_d=args.diffloss_d,\n",
    "        diffloss_w=args.diffloss_w,\n",
    "        num_sampling_steps=args.num_sampling_steps,\n",
    "        diffusion_batch_mul=args.diffusion_batch_mul,\n",
    "        grad_checkpointing=args.grad_checkpointing,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Load Diffusion model checkpoint (if provided)\n",
    "    if args.diffusion_ckpt and os.path.isfile(args.diffusion_ckpt):\n",
    "        print(f\"Loading Diffusion model checkpoint from {args.diffusion_ckpt}\")\n",
    "        checkpoint = torch.load(args.diffusion_ckpt, map_location='cpu', weights_only=False)\n",
    "        \n",
    "        # Get model state dict (use model_ema if exists)\n",
    "        if 'model_ema' in checkpoint:\n",
    "            ckpt_state_dict = checkpoint['model_ema']\n",
    "        else:\n",
    "            ckpt_state_dict = checkpoint['model']\n",
    "        \n",
    "        pos_embed_keys = [\n",
    "            'encoder_pos_embed_learned',\n",
    "            'decoder_pos_embed_learned',\n",
    "            'diffusion_pos_embed_learned',\n",
    "        ]\n",
    "        \n",
    "        # Create new state dict excluding all keys to ignore\n",
    "        ckpt_state_dict = {k: v for k, v in ckpt_state_dict.items() \n",
    "                               if not any(pos_key in k for pos_key in pos_embed_keys)}\n",
    "        \n",
    "        # Load with strict=False to allow mismatch\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(ckpt_state_dict, strict=False)\n",
    "        \n",
    "        print(f\"Loaded successfully!\")\n",
    "        print(f\"Missing keys: {missing_keys}\")\n",
    "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "        \n",
    "    for param in vae2d.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Print the trainable parameters and relative ratio\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable_params}, Total params: {total_params}, Trainable ratio: {trainable_params/total_params:.2%}\")\n",
    "    \n",
    "    return model, vae2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(args, rgb_gt, generated_images):\n",
    "    # calculate the metric\n",
    "    metrics = {}\n",
    "    method_list = ['mar']\n",
    "    metric_list = ['mse','ssim','psnr','lpips']\n",
    "    for method_name in method_list:\n",
    "        metrics[method_name] = {}\n",
    "        for metric_name in metric_list:\n",
    "            metrics[method_name][metric_name] = AverageMeter()\n",
    "    for key in metric_list :\n",
    "        metrics['mar'][key].update(compute_img_metric(rgb_gt, generated_images, key))\n",
    "    output_path = args.output_dir + \"/metrics.txt\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    for method_name in method_list:\n",
    "        re_msg = ''\n",
    "        for metric_name in metric_list:\n",
    "            re_msg += metric_name + \": \" + \"{:.3f}\".format(metrics[method_name][metric_name].avg) + \"  \"\n",
    "        with open(output_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"{method_name}: {re_msg}\" + '\\n')\n",
    "        print(f\"{method_name}: {re_msg}\")\n",
    "\n",
    "\n",
    "def evaluate(model, vae2d, viz_data, device, epoch, step, log_writer, args, spike_converter=None):\n",
    "    \"\"\"Evaluate the model and generate visualization results\"\"\"\n",
    "    if not misc.is_main_process():\n",
    "        return\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    model_unwrapped = model\n",
    "    \n",
    "    # Get samples from the dataset for visualization\n",
    "    blur_images = viz_data['blur'].to(device)[:8]  # Use only the first 8 samples\n",
    "    rgb_gt = viz_data['rgb'].to(device)[:8]\n",
    "    \n",
    "    # Generate spike data directly on GPU\n",
    "    spike_stream = spike_converter.rgb_to_spike(rgb_gt)[:8]\n",
    "    \n",
    "    # Directory to save output images\n",
    "    output_dir = Path(args.output_dir) / 'generations'\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Get latent representations via VAE encoder\n",
    "    with torch.no_grad():\n",
    "        # Encode blurred RGB\n",
    "        rgb_blur_latent = vae2d.encode(blur_images).latent_dist.sample().mul_(args.vae_scale)\n",
    "        vae_recon_blur = vae2d.decode(rgb_blur_latent / args.vae_scale)[0]\n",
    "        rgb_gt_latent = vae2d.encode(rgb_gt).latent_dist.sample().mul_(args.vae_scale)\n",
    "        vae_recon_gt = vae2d.decode(rgb_gt_latent / args.vae_scale)[0]\n",
    "        \n",
    "        # Generate samples\n",
    "        generated_tokens = model_unwrapped.recon_tokens(\n",
    "            blur_latent = rgb_blur_latent, \n",
    "            spike_stream = spike_stream,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        no_spike_generated_tokens = model_unwrapped.recon_tokens(\n",
    "            blur_latent = rgb_blur_latent, \n",
    "            spike_stream=None,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        no_rgb_generated_tokens = model_unwrapped.recon_tokens(\n",
    "            blur_latent = None, \n",
    "            spike_stream = spike_stream,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # Decode generated latent representations\n",
    "        generated_images = vae2d.decode(generated_tokens / args.vae_scale)[0]\n",
    "        no_spike_generated_images = vae2d.decode(no_spike_generated_tokens / args.vae_scale)[0]\n",
    "        no_rgb_generated_images = vae2d.decode(no_rgb_generated_tokens / args.vae_scale)[0]\n",
    "        \n",
    "        # Normalize to valid range\n",
    "        blur_images = (blur_images + 1) / 2\n",
    "        rgb_gt = (rgb_gt + 1) / 2\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "        no_spike_generated_images = (no_spike_generated_images + 1) / 2\n",
    "        no_rgb_generated_images = (no_rgb_generated_images + 1) / 2\n",
    "        vae_recon_blur = (vae_recon_blur + 1) / 2\n",
    "        vae_recon_gt = (vae_recon_gt + 1) / 2\n",
    "        \n",
    "        # pick a spike frame from spike stream\n",
    "        spike_frame = rgb_gt*spike_stream[:, :3, :, :]\n",
    "        compute_metric(args, rgb_gt, generated_images)\n",
    "\n",
    "    # Create summary comparison image\n",
    "    comparison = torch.cat([\n",
    "        blur_images, \n",
    "        spike_frame,\n",
    "        vae_recon_blur,\n",
    "        no_spike_generated_images,\n",
    "        no_rgb_generated_images,\n",
    "        generated_images,\n",
    "        vae_recon_gt,\n",
    "        rgb_gt,\n",
    "        ], dim=0)\n",
    "    grid = make_grid(comparison, nrow=8, normalize=True, padding=2)\n",
    "    \n",
    "    if log_writer is not None:\n",
    "        log_writer.add_image('Generated/comparison', grid, epoch)\n",
    "    \n",
    "    # Save image using torchvision's save_image instead of matplotlib\n",
    "    save_image(grid, str(output_dir / f'comparison_epoch_{epoch}_{step}.png'))\n",
    "    \n",
    "    # Resume training mode\n",
    "    model.train()\n",
    "\n",
    "def load_data(args):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    train_loader = get_online_process_dataloader(\n",
    "        image_folder=args.data_root,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        img_h=args.img_h,\n",
    "        img_w=args.img_w,\n",
    "        shuffle=True,\n",
    "        distributed=args.distributed,\n",
    "        rank=args.rank,\n",
    "        world_size=args.world_size,\n",
    "        kernel_size=args.kernel_size,\n",
    "        blur_intensity=args.blur_intensity,\n",
    "        blur_samples=args.blur_samples\n",
    "    )\n",
    "    \n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    if dataset_size == 0:\n",
    "        raise ValueError(f\"Dataset is empty, please check path: {args.data_root}\")\n",
    "    \n",
    "    print(f\"Dataset loaded, total samples: {dataset_size}\")\n",
    "    \n",
    "    # Create spike converter\n",
    "    spike_converter = SpikeConverter(\n",
    "        photon_samples=args.photon_samples,\n",
    "        target_coverage=args.target_coverage,\n",
    "        smooth_sigma=args.smooth_sigma,\n",
    "        gamma=args.gamma\n",
    "    )\n",
    "    \n",
    "    # Get one sample to set frame resolution\n",
    "    data = next(iter(train_loader))\n",
    "    # Generate spike data from RGB\n",
    "    spike_data = spike_converter.rgb_to_spike(data['rgb'])\n",
    "    args.frame_resolution = spike_data.shape[1]\n",
    "    \n",
    "    # Print data shapes for verification\n",
    "    print(f\"Shape check:\")\n",
    "    print(f\"- RGB image: {data['rgb'].shape}\")\n",
    "    print(f\"- Blur image: {data['blur'].shape}\")\n",
    "    print(f\"- Spike data: {spike_data.shape}\")\n",
    "    \n",
    "    return train_loader, spike_converter\n",
    "def unwrap_model(model, distributed=True):\n",
    "    \"\"\"Unwrap model from DistributedDataParallel\"\"\"\n",
    "    if distributed and hasattr(model, 'module'):\n",
    "        return model.module\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f11303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeGenArgs:\n",
    "    def __init__(self):\n",
    "        # Basic training params\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 100\n",
    "        self.model = 'spike_mar_base'\n",
    "\n",
    "        # VAE parameters\n",
    "        self.img_h = 256\n",
    "        self.img_w = 256\n",
    "        self.rgb_vae_path = \"ostris/vae-kl-f8-d16\"\n",
    "        self.rgb_vae_embed_dim = 16\n",
    "        self.vae_stride = 8\n",
    "        self.patch_size = 1\n",
    "        self.vae_scale = 0.2325\n",
    "\n",
    "        # Generation parameters\n",
    "        self.eval_freq = 100\n",
    "        self.save_last_freq = 10\n",
    "        self.online_eval = False\n",
    "\n",
    "        # Optimizer parameters\n",
    "        self.weight_decay = 0.02\n",
    "        self.grad_checkpointing = False\n",
    "        self.lr = None\n",
    "        self.blr = 1e-4\n",
    "        self.min_lr = 0.0\n",
    "        self.lr_schedule = 'cosine'\n",
    "        self.warmup_epochs = 10\n",
    "\n",
    "        # MAR parameters\n",
    "        self.diffusion_ckpt = '/path/to/your/diffusion_checkpoint.pth'  # Change to your checkpoint path\n",
    "        self.grad_clip = 3.0\n",
    "        self.attn_dropout = 0.1\n",
    "        self.proj_dropout = 0.1\n",
    "        self.buffer_size = 64\n",
    "\n",
    "        # Diffusion loss parameters\n",
    "        self.diffloss_d = 6\n",
    "        self.diffloss_w = 1024\n",
    "        self.num_sampling_steps = \"100\"\n",
    "        self.diffusion_batch_mul = 1\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        # Dataset parameters\n",
    "        self.data_root = '/path/to/your/dataset'  # Change to your dataset path\n",
    "        self.scenes = None  # Expect list[str] if used\n",
    "        self.output_dir = './output/spikegen_pretrained'\n",
    "        self.log_dir = './output_dir'\n",
    "        self.device = 'cuda'\n",
    "        self.seed = 0\n",
    "        self.resume = ''\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.num_workers = 10\n",
    "        self.pin_mem = True\n",
    "\n",
    "        # Distributed training\n",
    "        self.world_size = 1\n",
    "        self.local_rank = -1\n",
    "        self.dist_on_itp = False\n",
    "        self.dist_url = 'env://'\n",
    "\n",
    "        # Image processing\n",
    "        self.kernel_size = 40\n",
    "        self.blur_intensity = 40.0\n",
    "        self.blur_samples = 8\n",
    "        self.photon_samples = 8\n",
    "        self.target_coverage = 0.1\n",
    "        self.smooth_sigma = 1.0\n",
    "        self.gamma = 2.0  # gamma correction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = SpikeGenArgs()\n",
    "# Initialize distributed training\n",
    "misc.init_distributed_mode(args)\n",
    "\n",
    "# print('Working directory: {}'.format(os.path.dirname(os.path.realpath(__file__))))\n",
    "# print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix random seed\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set data loader\n",
    "train_loader, spike_converter = load_data(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, vae2d = load_model(args)\n",
    "# Move to target device\n",
    "model.to(device)\n",
    "vae2d.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear cache to avoid OOM\n",
    "viz_data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de55e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, vae2d, viz_data, device, 0, -1, None, args, spike_converter)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e9d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e53d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GBA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
